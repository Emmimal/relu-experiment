# relu-experiment
PyTorch experiments showing what happens when you remove ReLU from a deep network â€” loss curves, gradient collapse, depth sweep, decision boundaries, and activation comparison on MNIST.
